# Tim's Stack: Dynamic cross plattform web app stack

## Use Case

This stack is meant for dynamic realtime web apps with mobile clients.
Backend changes can directly be send to clients using a live websocket connection, 
state in the client is managed with redux. 
This allowes automaticly updating all affected clients on any backend changes.

For the web-application setting the django backend dynamicly requests nextjs pages, 
this includes dynamic page data so we get full ssr for all pages.

The nextjs frontend is integrated with capacitor and directly exports to android and ios.
In a native setting the frontend will try to request user data from the backend, 
if it fails it can fallback to a chached version allowing the user to view the full state of the app in an 'offline' mode.

## Stack Components

- nextjs + react frontend (deployment)
    - tailwind + dasyui 
    - automatic plattform adjustments for api calls, authentication and native functions (my custom implementation)
    - global redis store + auto background update websocket
    - capacitor setup for native integrations and ios / android pwa export

- django backend (deployment)
    - celery for tasks management ( or for offloading time intensive tasks )
    - rest_framework + django_rest_dataclasses for rapid REST api development
    - django proxy for authenticating views on other pods like the docs
    - drf_spectacular for autogenerated api documentation
    - django channels for managing websockets and sending update to clinets

- documentation (deployment)
    - pdoc3 code documentation generated from backend code
    
- postgresql (helm chart)
    - main backend database

- redis (helm chart)
    - broker for celery
    - db for django channels
    
## Usage
    
This outlines manual usage, for usage with bunnyshell.com check the section above.
    
### Local Development
    
For full local development you have the choice between running the full stack withing `microk8s` locally, or using `docker` and the `Makefile` to run individual components.

#### Microk8s local development

> This is harder to debug but allowes testing microservice interactions

This will spinup the stack configured for local development (`helm/values.yaml`), this is configured to mount directoryies `back/` and `front/` directly into the respecive containers to you will have hot code reloading!

1. setup `microk8s`: `make microk8s_setup`
2. build images and push them to the local microk8s registry: `make full_build_deploy`
3. Install the helm chart: `make helm_install` ( or use `make helm_update` to update an existing chart installation)

> You can switch micrk8s off when your finished with development `microk8s stop`

#### Docker local development

> This is the simples way to develop and debug, but has some differences in local routing strategies

Also mounts `front/` and `back/` directory. It also configures the host route `host.docker.internal:host-gateway` so that containers can interact with each other.

1. Build all images `make full_build`
2. Start all containers `make start_all` ( or start individual ones `make frontend_run`, `make backend_run`, `make start_redis`)

### Production Setup

You can configure any container registry by updating `Makefile` and `helm/values.yaml` I currently like using the github continer registry, the helm chart is setup to authorize pulling images from a github container registry.

You can build all production images **without** any of the production secrets they are only required for the deployed containers. To build all production images use `make full_build_prod` (TODO).

Now you need to authorize your docker installation to push to your private registry, with github container registry you can use `make authorize_github_push gha_token=<your-token>`.

Now you can push the build production images `make push_prod`.

For deployment create a copy of `helm/values.yaml -> heml/production-values.yaml` the choose secure usernames and password for all the services. Now you can install the helm chart on your cluster.